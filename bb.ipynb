{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I tried a simple pre-processing",
   "id": "cc6407f5d844f9a1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#this one works best\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Regular expression to match words, including splitting hyphenated words\n",
    "    tokens = regexp_tokenize(text, pattern=r'\\s|[\\.,;\\'\\\"\\-!\\*]', gaps=True)\n",
    "\n",
    "    # Split hyphenated words into separate words\n",
    "    fixed_tokens = []\n",
    "    for token in tokens:\n",
    "        if '-' in token:  # Split tokens with hyphens into separate words\n",
    "            fixed_tokens.extend(token.split('-'))\n",
    "        else:\n",
    "            fixed_tokens.append(token)\n",
    "\n",
    "    # Lowercasing and removing punctuation and stopwords\n",
    "    filtered_tokens = [\n",
    "        word.lower() for word in fixed_tokens\n",
    "        if word.isalnum() and word.lower() not in stop_words\n",
    "    ]\n",
    "\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# Assuming your articles are in a folder called \"news_articles\"\n",
    "folder_path = \"news_articles\"\n",
    "preprocessed_texts = []\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Read the file and preprocess the text\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        cleaned_text = preprocess_text(text)\n",
    "        preprocessed_texts.append(cleaned_text)\n",
    "\n",
    "# Print an example of the preprocessed text (for the first article)\n",
    "print(preprocessed_texts[1])  # prints the selected processed article for testing\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Boolean Model\n",
    "\n",
    "Due to the coding taking a lot of space not all words show up and it says \"The Jupyter server will temporarily stop sending output\n",
    "to the client in order to avoid crashing it.\""
   ],
   "id": "348161addd4a64d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Preprocess the text (tokenization, stopword removal)\n",
    "def preprocess(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]  # Remove stopwords and non-alphanumeric words\n",
    "    return filtered_tokens\n",
    "\n",
    "# Load all .txt files into a dictionary\n",
    "def load_documents(directory):\n",
    "    documents = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                documents[filename] = preprocess(file.read())\n",
    "    return documents\n",
    "\n",
    "# Build the Boolean term-document matrix\n",
    "def build_boolean_model(documents):\n",
    "    all_terms = set()\n",
    "    for doc in documents.values():\n",
    "        all_terms.update(doc)\n",
    "\n",
    "    term_document_matrix = {}\n",
    "    for term in all_terms:\n",
    "        term_document_matrix[term] = {}\n",
    "        for doc_name, doc_terms in documents.items():\n",
    "            term_document_matrix[term][doc_name] = 1 if term in doc_terms else 0\n",
    "    return term_document_matrix\n",
    "\n",
    "# Example usage\n",
    "directory = 'news_articles'  # Replace with the directory containing your .txt files\n",
    "documents = load_documents(directory)\n",
    "boolean_model = build_boolean_model(documents)\n",
    "\n",
    "# Print the Boolean Model for a sample term\n",
    "for term, doc_dict in boolean_model.items():\n",
    "    print(f\"Term: {term}\")\n",
    "    for doc_name, presence in doc_dict.items():\n",
    "        print(f\"  {doc_name}: {presence}\")\n"
   ],
   "id": "d41e0fd9ee8b3b3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another approach for the same Boolean Model (I wanted to just test it)",
   "id": "2831933df9614fd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to preprocess the text (tokenization, removing punctuation, lowercase)\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and lowercase all words\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Read the files from the news_articles directory\n",
    "directory = \"news_articles\"\n",
    "docs = []\n",
    "doc_names = []\n",
    "\n",
    "# Read all text files and preprocess them\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            processed_tokens = preprocess_text(content)\n",
    "            docs.append(processed_tokens)\n",
    "            doc_names.append(filename)\n",
    "\n",
    "# Create a vocabulary (set of unique terms)\n",
    "vocab = set()\n",
    "for doc in docs:\n",
    "    vocab.update(doc)\n",
    "\n",
    "# Sort the vocabulary alphabetically\n",
    "sorted_vocab = sorted(vocab)\n",
    "\n",
    "\n",
    "\n",
    "# Create the Boolean term-document matrix\n",
    "boolean_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Fill the matrix with 1s and 0s (1 if the term exists in the document, 0 otherwise)\n",
    "for doc_idx, doc in enumerate(docs):\n",
    "    for term in doc:\n",
    "        if term in sorted_vocab:\n",
    "            boolean_matrix[term][doc_names[doc_idx]] = 1\n",
    "\n",
    "# Print the Boolean Model\n",
    "print(\"\\nBoolean Model (Term-Document Matrix):\")\n",
    "for term in sorted_vocab:\n",
    "    print(f\"Term: {term}\")\n",
    "    for doc_name in doc_names:\n",
    "        print(f\"  {doc_name}: {boolean_matrix[term].get(doc_name, 0)}\")\n"
   ],
   "id": "7c293e7b9feec313",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inverted Boolean Model - This way would be more efficient",
   "id": "429be9eb73ad3130"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Basic Preprocess function (tokenize, lowercase, and remove non-alphanumeric)\n",
    "def preprocess_basic(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum()]  # Remove non-alphanumeric words\n",
    "    return filtered_tokens\n",
    "\n",
    "# Load all .txt files into a dictionary\n",
    "def load_documents(directory):\n",
    "    documents = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                documents[filename] = preprocess_basic(file.read())\n",
    "    return documents\n",
    "\n",
    "# Build the Inverted Index (Boolean model)\n",
    "def build_inverted_index(documents):\n",
    "    inverted_index = {}\n",
    "\n",
    "    for doc_name, terms in documents.items():\n",
    "        for term in terms:\n",
    "            if term not in inverted_index:\n",
    "                inverted_index[term] = []\n",
    "            # Add document to the inverted index list if term is present in that document\n",
    "            if doc_name not in inverted_index[term]:\n",
    "                inverted_index[term].append(doc_name)\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "# Display the Inverted Index in a readable format\n",
    "def display_inverted_index(inverted_index):\n",
    "    for term, doc_list in inverted_index.items():\n",
    "        print(f\"Term: {term}\")\n",
    "        print(f\"  Documents: {', '.join(doc_list)}\")\n",
    "\n",
    "# Example usage\n",
    "directory = 'news_articles'  # Path to your news articles folder\n",
    "documents = load_documents(directory)\n",
    "inverted_index = build_inverted_index(documents)\n",
    "display_inverted_index(inverted_index)\n"
   ],
   "id": "6e8f244af88e72fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TF-based Vector Space Model",
   "id": "a8f5fe3741996222"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to preprocess the text (tokenization, removing punctuation, lowercase)\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and lowercase all words\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Read the files from the news_articles directory\n",
    "directory = \"news_articles\"\n",
    "docs = []\n",
    "doc_names = []\n",
    "\n",
    "# Read all text files and preprocess them\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            processed_tokens = preprocess_text(content)\n",
    "            docs.append(processed_tokens)\n",
    "            doc_names.append(filename)\n",
    "\n",
    "# Create a vocabulary (set of unique terms)\n",
    "vocab = set()\n",
    "for doc in docs:\n",
    "    vocab.update(doc)\n",
    "\n",
    "# Sort the vocabulary alphabetically\n",
    "sorted_vocab = sorted(vocab)\n",
    "\n",
    "# Create the Term Frequency (TF) matrix\n",
    "tf_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Fill the matrix with term frequencies (count of term occurrences)\n",
    "for doc_idx, doc in enumerate(docs):\n",
    "    for term in doc:\n",
    "        if term in sorted_vocab:\n",
    "            tf_matrix[doc_names[doc_idx]][term] += 1\n",
    "\n",
    "# Print the Term Frequency (TF) Matrix\n",
    "print(\"\\nVector Space Model (Term Frequency - TF):\")\n",
    "for doc_name in doc_names:\n",
    "    print(f\"\\nDocument: {doc_name}\")\n",
    "    for term in sorted_vocab:\n",
    "        print(f\"  {term}: {tf_matrix[doc_name].get(term, 0)}\")\n"
   ],
   "id": "11a599985eb7c6d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I tried this code with a single file to see the output better",
   "id": "fb5e99a5b1253de8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to preprocess the text (tokenization, removing punctuation, lowercase)\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and lowercase all words\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Read the files from the news_articles directory (single document in this case)\n",
    "directory = \"news_articles\"\n",
    "docs = []\n",
    "doc_names = []\n",
    "\n",
    "# Read and preprocess only the specific file (artc4.txt for example)\n",
    "single_doc_filename = \"artc3.txt\"  # Change this to the filename you want to test\n",
    "\n",
    "with open(os.path.join(directory, single_doc_filename), 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    processed_tokens = preprocess_text(content)\n",
    "    docs.append(processed_tokens)\n",
    "    doc_names.append(single_doc_filename)\n",
    "\n",
    "# Create a vocabulary (set of unique terms)\n",
    "vocab = set()\n",
    "for doc in docs:\n",
    "    vocab.update(doc)\n",
    "\n",
    "# Sort the vocabulary alphabetically\n",
    "sorted_vocab = sorted(vocab)\n",
    "\n",
    "# Create the Term Frequency (TF) matrix\n",
    "tf_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Fill the matrix with term frequencies (count of term occurrences)\n",
    "for doc_idx, doc in enumerate(docs):\n",
    "    for term in doc:\n",
    "        if term in sorted_vocab:\n",
    "            tf_matrix[doc_names[doc_idx]][term] += 1\n",
    "\n",
    "# Print the Term Frequency (TF) Matrix for the single document\n",
    "print(\"\\nVector Space Model (Term Frequency - TF) for single document:\")\n",
    "for doc_name in doc_names:\n",
    "    print(f\"\\nDocument: {doc_name}\")\n",
    "    for term in sorted_vocab:\n",
    "        print(f\"  {term}: {tf_matrix[doc_name].get(term, 0)}\")\n"
   ],
   "id": "5a6575e5d4b1ef4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TF-IDF (Term Frequency-Inverse Document Frequency) Vector Space Model",
   "id": "eaa2b6576ae65ca7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Preprocess the text by tokenizing and cleaning\n",
    "def preprocess(doc):\n",
    "    # Remove non-alphanumeric characters (optional, depending on your needs)\n",
    "    doc = re.sub(r'\\W+', ' ', doc.lower())\n",
    "    return doc.split()\n",
    "\n",
    "# Compute Term Frequency (TF)\n",
    "def compute_tf(doc):\n",
    "    tf = Counter(doc)\n",
    "    total_terms = len(doc)\n",
    "    return {word: count / total_terms for word, count in tf.items()}\n",
    "\n",
    "# Compute Inverse Document Frequency (IDF)\n",
    "def compute_idf(corpus):\n",
    "    N = len(corpus)\n",
    "    idf = {}\n",
    "    doc_count = {}\n",
    "    for doc in corpus:\n",
    "        words_in_doc = set(doc)\n",
    "        for word in words_in_doc:\n",
    "            if word not in doc_count:\n",
    "                doc_count[word] = 0\n",
    "            doc_count[word] += 1\n",
    "\n",
    "    for word, count in doc_count.items():\n",
    "        idf[word] = math.log(N / count)\n",
    "\n",
    "    return idf\n",
    "\n",
    "# Compute TF-IDF\n",
    "def compute_tfidf(corpus):\n",
    "    tfidf = []\n",
    "    idf = compute_idf(corpus)\n",
    "    for doc in corpus:\n",
    "        tf = compute_tf(doc)\n",
    "        tfidf_doc = {word: tf[word] * idf[word] for word in tf}\n",
    "        tfidf.append(tfidf_doc)\n",
    "    return tfidf\n",
    "\n",
    "# Load the text files from the 'news_articles' directory\n",
    "def load_documents(directory):\n",
    "    docs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "                doc = f.read()\n",
    "                docs.append(preprocess(doc))\n",
    "    return docs\n",
    "\n",
    "# Example usage:\n",
    "# Assuming your files are located in the 'news_articles' directory\n",
    "docs = load_documents('news_articles')\n",
    "tfidf = compute_tfidf(docs)\n",
    "\n",
    "# Print the TF-IDF results for each document\n",
    "for idx, doc_tfidf in enumerate(tfidf):\n",
    "    print(f\"Article {idx + 1} :\")\n",
    "    print(doc_tfidf)\n",
    "    print(\"\\n\")\n"
   ],
   "id": "3278af4b01c53382",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I tried it for one text to see the output better:",
   "id": "f335a9d127e151ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'news_articles/artc1.txt'\n",
    "\n",
    "# Check if the file exists and read the content\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Apply vectorization to the text\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform([text])  # Only the single document\n",
    "\n",
    "    # Get the feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a dictionary of words and their corresponding TF-IDF scores\n",
    "    tfidf_dict = {word: float(score) for word, score in zip(feature_names, X.toarray()[0])}\n",
    "\n",
    "    # Output the dictionary\n",
    "    print(tfidf_dict)\n",
    "else:\n",
    "    print(f\"File '{file_path}' not found.\")\n"
   ],
   "id": "9a3c9ae6790e5b19",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
